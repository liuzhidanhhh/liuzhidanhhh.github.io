<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta property="og:type" content="website">
<meta property="og:title" content="Lzd&#39;s blog">
<meta property="og:url" content="https://liuzhidanhhh.github.io/index.html">
<meta property="og:site_name" content="Lzd&#39;s blog">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Lzd&#39;s blog">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://liuzhidanhhh.github.io/"/>





  <title>Lzd's blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Lzd's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://liuzhidanhhh.github.io/2018/07/29/GAN-serial/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="lzd">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lzd's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/29/GAN-serial/" itemprop="url">GAN serial</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-07-29T23:57:15+08:00">
                2018-07-29
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>论文链接： <a href="https://arxiv.org/abs/1701.07875" target="_blank" rel="noopener">WGAN</a> 、  <a href="https://arxiv.org/abs/1511.06434" target="_blank" rel="noopener">DCGAN</a> 、<a href="https://arxiv.org/abs/1806.01875" target="_blank" rel="noopener">EEG GAN</a></p>
<p>在上一篇文章中中详细介绍了GAN 公式推导及优缺点。</p>
<p>在这文章里主要介绍WGAN、DCGAN、EEG GAN 的特点，对于GAN的改进。</p>
<p>参考了：<a href="https://zhuanlan.zhihu.com/p/25071913" target="_blank" rel="noopener">令人拍案叫绝的Wasserstein GAN</a> 对于WGAN的介绍非常清晰，强推！</p>
<h3 id="GAN的缺点"><a href="#GAN的缺点" class="headerlink" title="GAN的缺点"></a>GAN的缺点</h3><p>●  训练GAN需要达到纳什均衡,有时候可以用梯度下降法做到,有时候做不到。我们还没有找到很好的达到纳什均衡的方法,所以训练GAN相比VAE或者PixelRNN是不稳定的,但在实践中它还是比训练玻尔兹曼机稳定。</p>
<p>●  GAN不适合处理离散形式的数据，比如文本。</p>
<p>●  GAN存在训练不稳定、梯度消失、模式崩溃、生成样本缺乏多样性等问题。</p>
<h4 id="第一种原始GAN存在的问题"><a href="#第一种原始GAN存在的问题" class="headerlink" title="第一种原始GAN存在的问题"></a>第一种原始GAN存在的问题</h4><p><strong>梯度消失GAN不稳定问题判别器越好，生成器梯度消失越严重。</strong></p>
<p>原始GAN中判别器要最小化的损失函数：</p>
<p><img src="https://www.zhihu.com/equation?tex=-%5Cmathbb%7BE%7D_%7Bx%5Csim+P_r%7D%5B%5Clog+D%28x%29%5D+-+%5Cmathbb%7BE%7D_%7Bx%5Csim+P_g%7D%5B%5Clog%281-D%28x%29%29%5D" alt="-\mathbb{E}_{x\sim P_r}[\log D(x)] - \mathbb{E}_{x\sim P_g}[\log(1-D(x))]">（公式1 ） </p>
<p>在生成器G固定参数时最优的判别器D为：</p>
<p><img src="https://www.zhihu.com/equation?tex=D%5E%2A%28x%29+%3D+%5Cfrac%7BP_r%28x%29%7D%7BP_r%28x%29+%2B+P_g%28x%29%7D" alt="D^*(x) = \frac{P_r(x)}{P_r(x) + P_g(x)}">（公式4）</p>
<p>经过推导公式一可变换得到：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cmathbb%7BE%7D_%7Bx+%5Csim+P_r%7D+%5Clog+%5Cfrac%7BP_r%28x%29%7D%7B%5Cfrac%7B1%7D%7B2%7D%5BP_r%28x%29+%2B+P_g%28x%29%5D%7D+%2B+%5Cmathbb%7BE%7D_%7Bx+%5Csim+P_g%7D+%5Clog+%5Cfrac%7BP_g%28x%29%7D%7B%5Cfrac%7B1%7D%7B2%7D%5BP_r%28x%29+%2B+P_g%28x%29%5D%7D+-+2%5Clog+2" alt="\mathbb{E}_{x \sim P_r} \log \frac{P_r(x)}{\frac{1}{2}[P_r(x) + P_g(x)]} + \mathbb{E}_{x \sim P_g} \log \frac{P_g(x)}{\frac{1}{2}[P_r(x) + P_g(x)]} - 2\log 2">（公式5)</p>
<p>写成JS散度形式：</p>
<p><img src="https://www.zhihu.com/equation?tex=JS%28P_1+%7C%7C+P_2%29+%3D+%5Cfrac%7B1%7D%7B2%7DKL%28P_1%7C%7C%5Cfrac%7BP_1+%2B+P_2%7D%7B2%7D%29+%2B+%5Cfrac%7B1%7D%7B2%7DKL%28P_2%7C%7C%5Cfrac%7BP_1+%2B+P_2%7D%7B2%7D%29" alt="JS(P_1 || P_2) = \frac{1}{2}KL(P_1||\frac{P_1 + P_2}{2}) + \frac{1}{2}KL(P_2||\frac{P_1 + P_2}{2})">（公式7）</p>
<p>具体推导看我的上一篇博客，或者令人拍案叫绝的<a href="https://zhuanlan.zhihu.com/p/25071913" target="_blank" rel="noopener">Wasserstein GAN</a> </p>
<p>将原始GAN定义为生成器loss等价变换为最小化真实分布<img src="https://www.zhihu.com/equation?tex=P_r" alt="P_r">与生成分布<img src="https://www.zhihu.com/equation?tex=P_g" alt="P_g">之间的JS散度。我们越训练判别器，它就越接近最优，最小化生成器的loss也就会越近似于最小化<img src="https://www.zhihu.com/equation?tex=P_r" alt="P_r">和<img src="https://www.zhihu.com/equation?tex=P_g" alt="P_g">之间的JS散度。</p>
<p>我们希望如果两个分布之间越接近它们的JS散度越小，我们通过优化JS散度就能将<img src="https://www.zhihu.com/equation?tex=P_g" alt="P_g">“拉向”<img src="https://www.zhihu.com/equation?tex=P_r" alt="P_r">，最终以假乱真。这个希望在两个分布有所重叠的时候是成立的，但是如果两个分布完全没有重叠的部分，或者它们重叠的部分可忽略（下面解释什么叫可忽略），它们的JS散度是是<img src="https://www.zhihu.com/equation?tex=%5Clog+2" alt="\log 2"></p>
<p>因为对于任意一个x只有四种可能：</p>
<p><img src="https://www.zhihu.com/equation?tex=P_1%28x%29+%3D+0" alt="P_1(x) = 0">且<img src="https://www.zhihu.com/equation?tex=P_2%28x%29+%3D+0" alt="P_2(x) = 0"></p>
<p><img src="https://www.zhihu.com/equation?tex=P_1%28x%29+%5Cneq+0" alt="P_1(x) \neq 0">且<img src="https://www.zhihu.com/equation?tex=P_2%28x%29+%5Cneq+0" alt="P_2(x) \neq 0"></p>
<p><img src="https://www.zhihu.com/equation?tex=P_1%28x%29+%3D+0" alt="P_1(x) = 0">且<img src="https://www.zhihu.com/equation?tex=P_2%28x%29+%5Cneq+0" alt="P_2(x) \neq 0"></p>
<p><img src="https://www.zhihu.com/equation?tex=P_1%28x%29+%5Cneq+0" alt="P_1(x) \neq 0">且<img src="https://www.zhihu.com/equation?tex=P_2%28x%29+%3D+0" alt="P_2(x) = 0"></p>
<p>第一种对计算JS散度无贡献，第二种情况由于重叠部分可忽略所以贡献也为0，第三种情况对公式7右边第一个项的贡献是<img src="https://www.zhihu.com/equation?tex=%5Clog+%5Cfrac%7BP_2%7D%7B%5Cfrac%7B1%7D%7B2%7D%28P_2+%2B+0%29%7D+%3D+%5Clog+2" alt="\log \frac{P_2}{\frac{1}{2}(P_2 + 0)} = \log 2">，第四种情况与之类似，所以最终<img src="https://www.zhihu.com/equation?tex=JS%28P_1%7C%7CP_2%29+%3D+%5Clog+2" alt="JS(P_1||P_2) = \log 2">。</p>
<p>换句话说，无论<img src="https://www.zhihu.com/equation?tex=P_r" alt="P_r">跟<img src="https://www.zhihu.com/equation?tex=P_g%0A" alt="P_g ">是远在天边，还是近在眼前，只要它们俩没有一点重叠或者重叠部分可忽略，JS散度就固定是常数<img src="https://www.zhihu.com/equation?tex=%5Clog+2" alt="\log 2">，<strong>而这对于梯度下降方法意味着——梯度为0</strong>！此时对于最优判别器来说，生成器肯定是得不到一丁点梯度信息的；即使对于接近最优的判别器来说，生成器也有很大机会面临梯度消失的问题。但是<img src="https://www.zhihu.com/equation?tex=P_r" alt="P_r">与<img src="https://www.zhihu.com/equation?tex=P_g" alt="P_g">不重叠或重叠部分可忽略的可能性非常大。具体证明看链接。</p>
<p>判别器训练得太好，生成器梯度消失，生成器loss降不下去；判别器训练得不好，生成器梯度不准，四处乱跑。只有判别器训练得不好不坏才行，但是这个火候又很难把握，甚至在同一轮训练的前后不同阶段这个火候都可能不一样，所以GAN才那么难训练。</p>
<h4 id="第二种GAN存在的问题"><a href="#第二种GAN存在的问题" class="headerlink" title="第二种GAN存在的问题"></a>第二种GAN存在的问题</h4><p>第二种GAN把生成器loss改成</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cmathbb%7BE%7D_%7Bx%5Csim+P_g%7D%5B-+%5Clog+D%28x%29%5D" alt="\mathbb{E}_{x\sim P_g}[- \log D(x)]">（公式3 ）</p>
<p><strong>最小化第二种生成器loss函数，会等价于最小化一个不合理的距离衡量，导致两个问题，一是梯度不稳定，二是collapse mode即多样性不足。</strong></p>
<p>上文推导已经得到在最优判别器$D^*$下</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cmathbb%7BE%7D_%7Bx%5Csim+P_r%7D%5B%5Clog+D%5E%2A%28x%29%5D+%2B+%5Cmathbb%7BE%7D_%7Bx%5Csim+P_g%7D%5B%5Clog%281-D%5E%2A%28x%29%29%5D+%3D+2JS%28P_r+%7C%7C+P_g%29+-+2%5Clog+2" alt="\mathbb{E}_{x\sim P_r}[\log D^*(x)] + \mathbb{E}_{x\sim P_g}[\log(1-D^*(x))] = 2JS(P_r || P_g) - 2\log 2">（公式9）</p>
<p>我们可以把KL散度（注意下面是先g后r）变换成含$D^*$的形式：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D%0AKL%28P_g+%7C%7C+P_r%29+%26%3D+%5Cmathbb%7BE%7D_%7Bx+%5Csim+P_g%7D+%5B%5Clog+%5Cfrac%7BP_g%28x%29%7D%7BP_r%28x%29%7D%5D+%5C%5C%0A%26%3D+%5Cmathbb%7BE%7D_%7Bx+%5Csim+P_g%7D+%5B%5Clog+%5Cfrac%7BP_g%28x%29+%2F+%28P_r%28x%29+%2B+P_g%28x%29%29%7D%7BP_r%28x%29+%2F+%28P_r%28x%29+%2B+P_g%28x%29%29%7D%5D+%5C%5C%0A%26%3D+%5Cmathbb%7BE%7D_%7Bx+%5Csim+P_g%7D+%5B%5Clog+%5Cfrac%7B1+-+D%5E%2A%28x%29%7D%7BD%5E%2A%28x%29%7D%5D+%5C%5C%0A%26%3D+%5Cmathbb%7BE%7D_%7Bx+%5Csim+P_g%7D+%5Clog+%5B1+-+D%5E%2A%28x%29%5D+-++%5Cmathbb%7BE%7D_%7Bx+%5Csim+P_g%7D+%5Clog+D%5E%2A%28x%29%0A%5Cend%7Balign%7D+%5C%5C" alt="\begin{align} KL(P_g || P_r) &amp;= \mathbb{E}_{x \sim P_g} [\log \frac{P_g(x)}{P_r(x)}] \\ &amp;= \mathbb{E}_{x \sim P_g} [\log \frac{P_g(x) / (P_r(x) + P_g(x))}{P_r(x) / (P_r(x) + P_g(x))}] \\ &amp;= \mathbb{E}_{x \sim P_g} [\log \frac{1 - D^*(x)}{D^*(x)}] \\ &amp;= \mathbb{E}_{x \sim P_g} \log [1 - D^*(x)] -  \mathbb{E}_{x \sim P_g} \log D^*(x) \end{align} \\">（公式10）</p>
<p>由公式3，9，10可得最小化目标的等价变形</p>
<p>注意上式最后两项不依赖于生成器G，最终得到最小化公式3等价于最小化</p>
<p><img src="https://www.zhihu.com/equation?tex=KL%28P_g+%7C%7C+P_r%29+-+2JS%28P_r+%7C%7C+P_g%29" alt="KL(P_g || P_r) - 2JS(P_r || P_g)">（公式11）</p>
<p>这个等价最小化目标存在两个严重的问题。第一是它同时要最小化生成分布与真实分布的KL散度，却又要最大化两者的JS散度，一个要拉近，一个却要推远！这在直观上非常荒谬，在数值上则会导致梯度不稳定，这是后面那个JS散度项的毛病。</p>
<p>第二，即便是前面那个正常的KL散度项也有毛病。因为KL散度不是一个对称的衡量，<img src="https://www.zhihu.com/equation?tex=KL%28P_g+%7C%7C+P_r%29" alt="KL(P_g || P_r)">与<img src="https://www.zhihu.com/equation?tex=KL%28P_r+%7C%7C+P_g%29" alt="KL(P_r || P_g)">是有差别的。以前者为例</p>
<ul>
<li>当<img src="https://www.zhihu.com/equation?tex=P_g%28x%29%5Crightarrow+0" alt="P_g(x)\rightarrow 0">而<img src="https://www.zhihu.com/equation?tex=P_r%28x%29%5Crightarrow+1" alt="P_r(x)\rightarrow 1">时，<img src="https://www.zhihu.com/equation?tex=P_g%28x%29+%5Clog+%5Cfrac%7BP_g%28x%29%7D%7BP_r%28x%29%7D+%5Crightarrow+0" alt="P_g(x) \log \frac{P_g(x)}{P_r(x)} \rightarrow 0">，对<img src="https://www.zhihu.com/equation?tex=KL%28P_g+%7C%7C+P_r%29" alt="KL(P_g || P_r)">贡献趋近0</li>
<li>当<img src="https://www.zhihu.com/equation?tex=P_g%28x%29%5Crightarrow+1" alt="P_g(x)\rightarrow 1">而<img src="https://www.zhihu.com/equation?tex=P_r%28x%29%5Crightarrow+0" alt="P_r(x)\rightarrow 0">时，<img src="https://www.zhihu.com/equation?tex=P_g%28x%29+%5Clog+%5Cfrac%7BP_g%28x%29%7D%7BP_r%28x%29%7D+%5Crightarrow+%2B%5Cinfty" alt="P_g(x) \log \frac{P_g(x)}{P_r(x)} \rightarrow +\infty">，对<img src="https://www.zhihu.com/equation?tex=KL%28P_g+%7C%7C+P_r%29" alt="KL(P_g || P_r)">贡献趋近正无穷</li>
</ul>
<p>换言之，<img src="https://www.zhihu.com/equation?tex=KL%28P_g+%7C%7C+P_r%29" alt="KL(P_g || P_r)">对于上面两种错误的惩罚是不一样的，第一种错误对应的是“生成器没能生成真实的样本”，惩罚微小；第二种错误对应的是“生成器生成了不真实的样本” ，惩罚巨大。第一种错误对应的是缺乏多样性，第二种错误对应的是缺乏准确性。<strong>这一放一打之下，生成器宁可多生成一些重复但是很“安全”的样本，也不愿意去生成多样性的样本，因为那样一不小心就会产生第二种错误，得不偿失。这种现象就是大家常说的collapse mode。</strong></p>
<h3 id="WGAN"><a href="#WGAN" class="headerlink" title="WGAN"></a>WGAN</h3><p>论文<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1701.04862" target="_blank" rel="noopener">Towards Principled Methods for Training Generative Adversarial Networks</a> 从理论上分析了原始GAN的问题所在，并提出改进</p>
<p>论文<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1701.07875" target="_blank" rel="noopener">Wasserstein GAN</a> 给出了GAN 的改进算法WGAN。</p>
<h4 id="WGAN的优点"><a href="#WGAN的优点" class="headerlink" title="WGAN的优点"></a><strong>WGAN的优点</strong></h4><ul>
<li>彻底解决GAN训练不稳定的问题，不再需要小心平衡生成器和判别器的训练程度</li>
<li>基本解决了collapse mode的问题，确保了生成样本的多样性 </li>
<li>训练过程中终于有一个像交叉熵、准确率这样的数值来指示训练的进程，这个数值越小代表GAN训练得越好，代表生成器产生的图像质量越高（如题图所示）</li>
<li>以上一切好处不需要精心设计的网络架构，最简单的多层全连接网络就可以做到</li>
</ul>
<h4 id="WGAN算法流程中对GAN的改进"><a href="#WGAN算法流程中对GAN的改进" class="headerlink" title="WGAN算法流程中对GAN的改进"></a><strong>WGAN算法流程中对GAN的改进</strong></h4><ul>
<li>判别器最后一层去掉sigmoid</li>
<li>生成器和判别器的loss不取log</li>
<li>每次更新判别器的参数之后把它们的绝对值截断到不超过一个固定常数c</li>
<li>不要用基于动量的优化算法（包括momentum和Adam），推荐RMSProp，SGD也行</li>
</ul>
<h4 id="WGAN算法"><a href="#WGAN算法" class="headerlink" title="WGAN算法"></a><strong>WGAN算法</strong></h4><p><img src="/2018/07/29/GAN-serial/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-07-29%2018.11.15.png" alt="屏幕快照 2018-07-29 18.11.15"></p>
<h4 id="Wasserstein距离的优越性质"><a href="#Wasserstein距离的优越性质" class="headerlink" title="Wasserstein距离的优越性质"></a>Wasserstein距离的优越性质</h4><p>Wasserstein距离又叫Earth-Mover（EM）距离，定义如下：</p>
<p><img src="https://www.zhihu.com/equation?tex=W%28P_r%2C+P_g%29+%3D+%5Cinf_%7B%5Cgamma+%5Csim+%5CPi+%28P_r%2C+P_g%29%7D+%5Cmathbb%7BE%7D_%7B%28x%2C+y%29+%5Csim+%5Cgamma%7D+%5B%7C%7Cx+-+y%7C%7C%5D" alt="W(P_r, P_g) = \inf_{\gamma \sim \Pi (P_r, P_g)} \mathbb{E}_{(x, y) \sim \gamma} [||x - y||]">（公式12）</p>
<p>解释如下：<img src="https://www.zhihu.com/equation?tex=%5CPi+%28P_r%2C+P_g%29" alt="\Pi (P_r, P_g)">是<img src="https://www.zhihu.com/equation?tex=P_r" alt="P_r">和<img src="https://www.zhihu.com/equation?tex=P_g" alt="P_g">组合起来的所有可能的联合分布的集合，反过来说，<img src="https://www.zhihu.com/equation?tex=%5CPi+%28P_r%2C+P_g%29" alt="\Pi (P_r, P_g)">中每一个分布的边缘分布都是<img src="https://www.zhihu.com/equation?tex=P_r" alt="P_r">和<img src="https://www.zhihu.com/equation?tex=P_g" alt="P_g">。对于每一个可能的联合分布<img src="https://www.zhihu.com/equation?tex=%5Cgamma" alt="\gamma">而言，可以从中采样<img src="https://www.zhihu.com/equation?tex=%28x%2C+y%29+%5Csim+%5Cgamma" alt="(x, y) \sim \gamma">得到一个真实样本<img src="https://www.zhihu.com/equation?tex=x" alt="x">和一个生成样本<img src="https://www.zhihu.com/equation?tex=y" alt="y">，并算出这对样本的距离<img src="https://www.zhihu.com/equation?tex=%7C%7Cx-y%7C%7C" alt="||x-y||">，所以可以计算该联合分布<img src="https://www.zhihu.com/equation?tex=%5Cgamma" alt="\gamma">下样本对距离的期望值<img src="https://www.zhihu.com/equation?tex=%5Cmathbb%7BE%7D_%7B%28x%2C+y%29+%5Csim+%5Cgamma%7D+%5B%7C%7Cx+-+y%7C%7C%5D" alt="\mathbb{E}_{(x, y) \sim \gamma} [||x - y||]">。在所有可能的联合分布中能够对这个期望值取到的下界<img src="https://www.zhihu.com/equation?tex=%5Cinf_%7B%5Cgamma+%5Csim+%5CPi+%28P_r%2C+P_g%29%7D+%5Cmathbb%7BE%7D_%7B%28x%2C+y%29+%5Csim+%5Cgamma%7D+%5B%7C%7Cx+-+y%7C%7C%5D" alt="\inf_{\gamma \sim \Pi (P_r, P_g)} \mathbb{E}_{(x, y) \sim \gamma} [||x - y||]">，就定义为Wasserstein距离。</p>
<p>直观上可以把<img src="https://www.zhihu.com/equation?tex=%5Cmathbb%7BE%7D_%7B%28x%2C+y%29+%5Csim+%5Cgamma%7D+%5B%7C%7Cx+-+y%7C%7C%5D" alt="\mathbb{E}_{(x, y) \sim \gamma} [||x - y||]">理解为在<img src="https://www.zhihu.com/equation?tex=%5Cgamma" alt="\gamma">这个“路径规划”下把<img src="https://www.zhihu.com/equation?tex=P_r" alt="P_r">这堆“沙土”挪到<img src="https://www.zhihu.com/equation?tex=P_g" alt="P_g">“位置”所需的“消耗”，而<img src="https://www.zhihu.com/equation?tex=W%28P_r%2C+P_g%29" alt="W(P_r, P_g)">就是“最优路径规划”下的“最小消耗”，所以才叫Earth-Mover（推土机）距离。</p>
<p><strong>Wasserstein距离相比KL散度、JS散度的优越性在于，即便两个分布没有重叠，Wasserstein距离仍然能够反映它们的远近。</strong>WGAN本作通过简单的例子展示了这一点。考虑如下二维空间中的两个分布<img src="https://www.zhihu.com/equation?tex=P_1" alt="P_1">和<img src="https://www.zhihu.com/equation?tex=P_2" alt="P_2">，<img src="https://www.zhihu.com/equation?tex=P_1" alt="P_1">在线段AB上均匀分布，<img src="https://www.zhihu.com/equation?tex=P_2" alt="P_2">在线段CD上均匀分布，通过控制参数<img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="\theta">可以控制着两个分布的距离远近。</p>
<p><img src="https://pic3.zhimg.com/80/v2-c9cc9f8c879e7fe93d6e3bfafd41bd8a_hd.jpg" alt="img"></p>
<p>此时容易得到（读者可自行验证）</p>
<p><img src="https://www.zhihu.com/equation?tex=KL%28P_1+%7C%7C+P_2%29+%3D+KL%28P_1+%7C%7C+P_2%29+%3D%0A%5Cbegin%7Bcases%7D%0A%2B%5Cinfty+%26+%5Ctext%7Bif+%24%5Ctheta+%5Cneq+0%24%7D+%5C%5C%0A0+%26+%5Ctext%7Bif+%24%5Ctheta+%3D+0%24%7D%0A%5Cend%7Bcases%7D" alt="KL(P_1 || P_2) = KL(P_1 || P_2) = \begin{cases} +\infty &amp; \text{if $\theta \neq 0$} \\ 0 &amp; \text{if $\theta = 0$} \end{cases}">（突变）</p>
<p><img src="https://www.zhihu.com/equation?tex=JS%28P_1%7C%7CP_2%29%3D%0A%5Cbegin%7Bcases%7D%0A%5Clog+2+%26+%5Ctext%7Bif+%24%5Ctheta+%5Cneq+0%24%7D+%5C%5C%0A0+%26+%5Ctext%7Bif+%24%5Ctheta+-+0%24%7D%0A%5Cend%7Bcases%7D" alt="JS(P_1||P_2)= \begin{cases} \log 2 &amp; \text{if $\theta \neq 0$} \\ 0 &amp; \text{if $\theta - 0$} \end{cases}">（突变）</p>
<p><img src="https://www.zhihu.com/equation?tex=W%28P_0%2C+P_1%29+%3D+%7C%5Ctheta%7C" alt="W(P_0, P_1) = |\theta|">（平滑）</p>
<p>KL散度和JS散度是突变的，要么最大要么最小，<strong>Wasserstein距离却是平滑的</strong>，如果我们要用梯度下降法优化<img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="\theta">这个参数，前两者根本提供不了梯度，Wasserstein距离却可以。类似地，在高维空间中如果两个分布不重叠或者重叠部分可忽略，则KL和JS既反映不了远近，也提供不了梯度，<strong>但是Wasserstein却可以提供有意义的梯度</strong>。</p>
<p>由于Wasserstein距离定义（公式12）中的<img src="https://www.zhihu.com/equation?tex=%5Cinf_%7B%5Cgamma+%5Csim+%5CPi+%28P_r%2C+P_g%29%7D" alt="\inf_{\gamma \sim \Pi (P_r, P_g)}">没法直接求解，不过没关系，作者用了一个已有的定理把它变换为如下形式</p>
<p><img src="https://www.zhihu.com/equation?tex=W%28P_r%2C+P_g%29+%3D+%5Cfrac%7B1%7D%7BK%7D+%5Csup_%7B%7C%7Cf%7C%7C_L+%5Cleq+K%7D+%5Cmathbb%7BE%7D_%7Bx+%5Csim+P_r%7D+%5Bf%28x%29%5D+-+%5Cmathbb%7BE%7D_%7Bx+%5Csim+P_g%7D+%5Bf%28x%29%5D" alt="W(P_r, P_g) = \frac{1}{K} \sup_{||f||_L \leq K} \mathbb{E}_{x \sim P_r} [f(x)] - \mathbb{E}_{x \sim P_g} [f(x)]">（公式13）</p>
<p>首先需要介绍一个概念——Lipschitz连续。它其实就是在一个连续函数<img src="https://www.zhihu.com/equation?tex=f" alt="f">上面额外施加了一个限制，要求存在一个常数<img src="https://www.zhihu.com/equation?tex=K%5Cgeq+0" alt="K\geq 0">使得定义域内的任意两个元素<img src="https://www.zhihu.com/equation?tex=x_1" alt="x_1">和<img src="https://www.zhihu.com/equation?tex=x_2" alt="x_2">都满足</p>
<p>此时称函数<img src="https://www.zhihu.com/equation?tex=f" alt="f">的Lipschitz常数为<img src="https://www.zhihu.com/equation?tex=K" alt="K">。</p>
<p>简单理解，比如说<img src="https://www.zhihu.com/equation?tex=f" alt="f">的定义域是实数集合，那上面的要求就等价于<img src="https://www.zhihu.com/equation?tex=f" alt="f">的导函数绝对值不超过<img src="https://www.zhihu.com/equation?tex=K" alt="K">。再比如说<img src="https://www.zhihu.com/equation?tex=%5Clog+%28x%29" alt="\log (x)">就不是Lipschitz连续，因为它的导函数没有上界。Lipschitz连续条件限制了一个连续函数的最大局部变动幅度。</p>
<p>公式13的意思就是在要求函数<img src="https://www.zhihu.com/equation?tex=f" alt="f">的Lipschitz常数<img src="https://www.zhihu.com/equation?tex=%7C%7Cf%7C%7C_L" alt="||f||_L">不超过<img src="https://www.zhihu.com/equation?tex=K" alt="K">的条件下，对所有可能满足条件的<img src="https://www.zhihu.com/equation?tex=f" alt="f">取到<img src="https://www.zhihu.com/equation?tex=%5Cmathbb%7BE%7D_%7Bx+%5Csim+P_r%7D+%5Bf%28x%29%5D+-+%5Cmathbb%7BE%7D_%7Bx+%5Csim+P_g%7D+%5Bf%28x%29%5D" alt="\mathbb{E}_{x \sim P_r} [f(x)] - \mathbb{E}_{x \sim P_g} [f(x)]">的上界，然后再除以<img src="https://www.zhihu.com/equation?tex=K" alt="K">。特别地，我们可以用一组参数<img src="https://www.zhihu.com/equation?tex=w" alt="w">来定义一系列可能的函数<img src="https://www.zhihu.com/equation?tex=f_w" alt="f_w">，此时求解公式13可以近似变成求解如下形式</p>
<p><img src="https://www.zhihu.com/equation?tex=K+%5Ccdot+W%28P_r%2C+P_g%29+%5Capprox+%5Cmax_%7Bw%3A+%7Cf_w%7C_L+%5Cleq+K%7D+%5Cmathbb%7BE%7D_%7Bx+%5Csim+P_r%7D+%5Bf_w%28x%29%5D+-+%5Cmathbb%7BE%7D_%7Bx+%5Csim+P_g%7D+%5Bf_w%28x%29%5D" alt="K \cdot W(P_r, P_g) \approx \max_{w: |f_w|_L \leq K} \mathbb{E}_{x \sim P_r} [f_w(x)] - \mathbb{E}_{x \sim P_g} [f_w(x)]">（公式14）</p>
<p>再用上我们搞深度学习的人最熟悉的那一套，不就可以把<img src="https://www.zhihu.com/equation?tex=f" alt="f">用一个带参数<img src="https://www.zhihu.com/equation?tex=w" alt="w">的神经网络来表示嘛！由于神经网络的拟合能力足够强大，我们有理由相信，这样定义出来的一系列<img src="https://www.zhihu.com/equation?tex=f_w" alt="f_w">虽然无法囊括所有可能，但是也足以高度近似公式13要求的那个<img src="https://www.zhihu.com/equation?tex=sup_%7B%7C%7Cf%7C%7C_L+%5Cleq+K%7D+" alt="sup_{||f||_L \leq K} ">了。</p>
<p>最后，还不能忘了满足公式14中<img src="https://www.zhihu.com/equation?tex=%7C%7Cf_w%7C%7C_L+%5Cleq+K" alt="||f_w||_L \leq K">这个限制。我们其实不关心具体的K是多少，只要它不是正无穷就行，因为它只是会使得梯度变大<img src="https://www.zhihu.com/equation?tex=K" alt="K">倍，并不会影响梯度的方向。所以作者采取了一个非常简单的做法，就是限制神经网络<img src="https://www.zhihu.com/equation?tex=f_%5Ctheta" alt="f_\theta">的所有参数<img src="https://www.zhihu.com/equation?tex=w_i" alt="w_i">的不超过某个范围<img src="https://www.zhihu.com/equation?tex=%5B-c%2C+c%5D" alt="[-c, c]">，比如<img src="https://www.zhihu.com/equation?tex=w_i+%5Cin+%5B-+0.01%2C+0.01%5D" alt="w_i \in [- 0.01, 0.01]">，此时关于输入样本<img src="https://www.zhihu.com/equation?tex=x" alt="x">的导数<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+f_w%7D%7B%5Cpartial+x%7D" alt="\frac{\partial f_w}{\partial x}">也不会超过某个范围，所以一定存在某个不知道的常数<img src="https://www.zhihu.com/equation?tex=K" alt="K">使得<img src="https://www.zhihu.com/equation?tex=f_w" alt="f_w">的局部变动幅度不会超过它，Lipschitz连续条件得以满足。具体在算法实现中，只需要每次更新完<img src="https://www.zhihu.com/equation?tex=w" alt="w">后把它clip回这个范围就可以了。</p>
<p><strong>到此为止，我们可以构造一个含参数<img src="https://www.zhihu.com/equation?tex=w" alt="w">、最后一层不是非线性激活层的判别器网络<img src="https://www.zhihu.com/equation?tex=f_w" alt="f_w">，在限制<img src="https://www.zhihu.com/equation?tex=w" alt="w">不超过某个范围的条件下，使得</strong></p>
<p><img src="https://www.zhihu.com/equation?tex=L+%3D+%5Cmathbb%7BE%7D_%7Bx+%5Csim+P_r%7D+%5Bf_w%28x%29%5D+-+%5Cmathbb%7BE%7D_%7Bx+%5Csim+P_g%7D+%5Bf_w%28x%29%5D(公式15" alt="L = \mathbb{E}_{x \sim P_r} [f_w(x)] - \mathbb{E}_{x \sim P_g} [f_w(x)]">)</p>
<p><strong>尽可能取到最大，此时<img src="https://www.zhihu.com/equation?tex=L" alt="L">就会近似真实分布与生成分布之间的Wasserstein距离（忽略常数倍数<img src="https://www.zhihu.com/equation?tex=K" alt="K">）。注意原始GAN的判别器做的是真假二分类任务，所以最后一层是sigmoid，但是现在WGAN中的判别器<img src="https://www.zhihu.com/equation?tex=f_w" alt="f_w">做的是近似拟合Wasserstein距离，属于回归任务，所以要把最后一层的sigmoid拿掉。</strong></p>
<p><strong>接下来生成器要近似地最小化Wasserstein距离，可以最小化<img src="https://www.zhihu.com/equation?tex=L" alt="L">，由于Wasserstein距离的优良性质，我们不需要担心生成器梯度消失的问题。再考虑到<img src="https://www.zhihu.com/equation?tex=L" alt="L">的第一项与生成器无关，就得到了WGAN的两个loss。</strong></p>
<p><img src="/2018/07/29/GAN-serial/公式16，WGAN生成器loss函数" alt="- \mathbb{E}_{x \sim P_g} [f_w(x)]](https://www.zhihu.com/equation?tex=-+%5Cmathbb%7BE%7D_%7Bx+%5Csim+P_g%7D+%5Bf_w%28x%29%5D"> )</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cmathbb%7BE%7D_%7Bx+%5Csim+P_g%7D+%5Bf_w%28x%29%5D-+%5Cmathbb%7BE%7D_%7Bx+%5Csim+P_r%7D+%5Bf_w%28x%29%5D(公式17，WGAN判别器loss函数" alt="\mathbb{E}_{x \sim P_g} [f_w(x)]- \mathbb{E}_{x \sim P_r} [f_w(x)]">)</p>
<p><strong>公式15是公式17的反，可以指示训练进程，其数值越小，表示真实分布与生成分布的Wasserstein距离越小，GAN训练得越好。</strong></p>
<h4 id="WGAN-存在的问题"><a href="#WGAN-存在的问题" class="headerlink" title="WGAN 存在的问题"></a>WGAN 存在的问题</h4><p>1、WGAN 的Lipschitz连续条件将权重w限制在一个K内，可能会导致神经网络的参数收敛到limit 的边界。</p>
<p>2、若生成网络和判别网络只用简单的2层感知机，信号特征提取不够充分。</p>
<h3 id="DCGAN"><a href="#DCGAN" class="headerlink" title="DCGAN"></a>DCGAN</h3><p><strong>DCGAN优点</strong></p>
<p>◆  为GAN的训练提供了一个很好的网络拓扑结构。</p>
<p>◆  表明生成的特征具有向量的计算特性。</p>
<p><strong>DCGAN能改进GAN训练稳定的原因主要有：</strong></p>
<p>◆  使用步长卷积代替上采样层，卷积在提取图像特征上具有很好的作用，并且使用卷积代替全连接层。</p>
<p>◆  生成器G和判别器D中几乎每一层都使用batchnorm层，将特征层的输出归一化到一起，加速了训练，提升了训练的稳定性。（生成器的最后一层和判别器的第一层不加batchnorm）</p>
<p>◆  在判别器中使用leakrelu激活函数，而不是RELU，防止梯度稀疏，生成器中仍然采用relu，但是输出层采用tanh</p>
<p>◆  使用adam优化器训练，并且学习率最好是0.0002，（我也试过其他学习率，不得不说0.0002是表现最好的了）</p>
<p><strong>DCGAN生成网络结构：</strong></p>
<p><img src="/2018/07/29/GAN-serial/dcgan0.png" alt="dcgan0"></p>
<p>DCGAN的生成器网络结构如上图所示，相较原始的GAN，DCGAN几乎完全使用了卷积层代替全链接层，判别器几乎是和生成器对称的，从上图中我们可以看到，整个网络没有pooling层和上采样层的存在，实际上是使用了带步长（fractional-strided）的卷积代替了上采样，以增加训练的稳定性。</p>
<p><strong>DCGAN的向量的计算特性</strong></p>
<p><img src="/2018/07/29/GAN-serial/dcgan1.png" alt="dcgan1"></p>
<p><strong>DCGAN 存在的问题</strong></p>
<p>GAN训练稳定性来说是治标不治本，没有从根本上解决问题，而且训练的时候仍需要小心的平衡G,D的训练进程，往往是训练一个多次，训练另一个一次。</p>
<h3 id="EEG-GAN"><a href="#EEG-GAN" class="headerlink" title="EEG GAN"></a>EEG GAN</h3><p>将GAN框架应用于人工脑电图信号的生成，提出了EEG-GAN框架，用于脑电图（electroencephalographic，EEG）脑信号的生成。</p>
<p><strong>特点：</strong> </p>
<p>1、改进WGAN 的距离函数，以稳定训练</p>
<p>2、采用了DCGAN 相似结构卷积进行上采样（up-sampling）和下采样（down-sampling） </p>
<p>能够有效的提取特征</p>
<p>3、加入了LSTM 适合做时间序列的数据的生成。</p>
<p>关于距离函数的改进，加入了惩罚项：<img src="/2018/07/29/GAN-serial/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-07-29%2023.05.35.png" alt="屏幕快照 2018-07-29 23.05.35"></p>
<p>其中，$\lambda$  不是随着Wasserstein距离的不同而做相应的变化，距离大 $\lambda$ 大，反正 $\lambda$ 小。具体原因见论文。</p>
<p>Loss 函数：<img src="/2018/07/29/GAN-serial/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-07-29%2023.10.34.png" alt="屏幕快照 2018-07-29 23.10.34"></p>
<p>该loss 的优点：当分布的距离较小时，仍然存在稳定的梯度。</p>
<p><strong>信号生成效果</strong></p>
<p><img src="/2018/07/29/GAN-serial/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-07-29%2023.05.44.png" alt="屏幕快照 2018-07-29 23.05.44"></p>
<p><strong>缺点</strong></p>
<p>今年刚出的论文，缺点还没有明显暴露；</p>
<p>没有源码，没有办法做快速验证。</p>
<p>参考：<a href="https://zhuanlan.zhihu.com/p/25071913" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/25071913</a></p>
<p><a href="https://blog.csdn.net/qq_25737169/article/details/78857788" target="_blank" rel="noopener">https://blog.csdn.net/qq_25737169/article/details/78857788</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://liuzhidanhhh.github.io/2018/07/24/GAN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="lzd">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lzd's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/24/GAN/" itemprop="url">GAN（Generative Adversarial Networks）</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-07-24T20:31:55+08:00">
                2018-07-24
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="GAN（Generative-Adversarial-Networks）"><a href="#GAN（Generative-Adversarial-Networks）" class="headerlink" title="GAN（Generative Adversarial Networks）"></a>GAN（Generative Adversarial Networks）</h2><p>论文链接：<a href="https://arxiv.org/abs/1406.2661" target="_blank" rel="noopener">https://arxiv.org/abs/1406.2661</a></p>
<h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>GAN 对抗式生成网络，主要是一种博弈思想。主要包含两个网络：判别网络D、生成网络G。</p>
<p>在这里判别网络可以看成是一个鉴赏家，生成网络可以看成是一个画家。判别网络D需要鉴定画作是出自新手画家G的，还是来自绘画大师的作品。G开始是一个新手画家，在和鉴赏家博弈的过程中，不断的提高自己的画技以欺骗鉴赏家，鉴赏家同时也在不断的提升自己的鉴赏能力。</p>
<h3 id="原理推导"><a href="#原理推导" class="headerlink" title="原理推导"></a>原理推导</h3><p>说明：原论文的推导跳过了一些细节，可以参考参考<a href="https://www.jiqizhixin.com/articles/2017-10-1-1" target="_blank" rel="noopener">机器之心</a>推导，但是机器之心有些地方的描述，比较生硬的翻译了原论文，不是很好理解，可以看原论文，下面的原理部分是我结合两者，并加入了自己的理解。</p>
<h4 id="宏观上的理解"><a href="#宏观上的理解" class="headerlink" title="宏观上的理解"></a>宏观上的理解</h4><p>GAN实际上是一个D、G的极大极小博弈。存在一个真实分布$P_{data}(x)$  和生成分布 $P_g(x)$ ,生成器G的目的是让生成分布 $P_g(x)$ 靠近真实分布$P_{data} (x)$ ，而判别器D的目的是区分两个分布。</p>
<p>我们需要的是这个可以以假乱真的生成网络。最优情况是$P_g(x)=P_{data}(x)$ ,此时D(x)=0.5。</p>
<p>下图GAN 的原理示意图：</p>
<p><img src="/2018/07/24/GAN/GANdemo.png" alt="GANdemo"></p>
<p>黑色虚线的分布是真实分布，绿色的分布是生成分布，蓝色虚线是判别分布，z是随机噪声，通过生成器将z映射到数据空间 x=G(z)，GAN会训练并更新判别分布（蓝色），指引生成分布向真实分布靠拢。</p>
<p>D、G的最大最小博弈值函数函数V(G,D):</p>
<p><img src="/2018/07/24/GAN/value%20function.png" alt="value function"></p>
<p>D(x)表示x来自真实数据集，而非生成数据集。训练D以最好的区分样本来自真实数据和生成数据，也就是最大化D(x),同时训练G以最小化log(1-D(G(z))), 可直观的理解为训练G生成的质量来使得D(G(z)) 最大，也就是log(1-D(G(z)))最小。</p>
<p>原论文表明在非参数限制的情况下可以得到最优解，这里的非参数限制，我不是很理解。</p>
<h4 id="GAN算法："><a href="#GAN算法：" class="headerlink" title="GAN算法："></a>GAN算法：</h4><p><img src="/2018/07/24/GAN/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-07-29%2010.43.54.png" alt="屏幕快照 2018-07-29 10.43.54"></p>
<p>说明：</p>
<ol>
<li>在收敛点附近的对抗训练$P_r$ 和$P_g$  相似，D是一个局部准确的分类器。</li>
<li>在算法先先训练k次D，再训练G，在k次内循环中，训练D来区分$P_{data}$ 和$P_g$ ，收敛到 $D^*(x)=\frac{P_{data}(x)}{P_{data}(x)+P_g(x)}$ </li>
<li>接着固定D，训练G，D的梯度引导着G的生成分布向真实分布靠近。</li>
<li>在复杂度足够的情况下(什么情况是足够的？)D、G会达到一个均衡点 $P_g(x)=P_{data}(x)$ ,此时D(x)=0.5。</li>
</ol>
<h4 id="GAN原理形式化的定义及说明"><a href="#GAN原理形式化的定义及说明" class="headerlink" title="GAN原理形式化的定义及说明"></a>GAN原理形式化的定义及说明</h4><p>GAN的目标是：生成器生成与真实数据几乎没有区别的样本，即将随机变量生成为某一种概率分布，也可以说概率密度函数为相等的。这可以通过构造最优化问题来解决，即定义一个最优化问题，其中最优生成器 G 满足$ P_G(x)=P_{data}(x)$。如果我们知道求解的 G 最后会满足该关系，那么我们就可以合理地期望神经网络通过典型的 SGD 训练就能得到最优的 G。</p>
<p>作者将最优化问题定义为一个值函数V(G&lt;D) ：</p>
<p><img src="/2018/07/24/GAN/value.png" alt="value"></p>
<p>其中第一项是定义一个判别器 D 以判别样本是不是从 $ P_{data}(x) $ 分布中取出来的，最大化这一项相当于令判别器 D 在 x 服从于 data 的概率密度时能准确地预测 D(x)=1。第二项是企图欺骗判别器的生成器 G，$D(G(z))\in(-1,1)$ , log(1-D(G(z)))&lt;0, 当G没有欺骗到D时，第二项的均值接近0，因此，判别器的目标为最大化上面值函数。</p>
<p>给定生成器G，最优判别器可以表示为：<br>$$<br>D^<em>_G=argmax_DV(G,D)<br>$$<br>当$D=D^</em>_G$ 固定，优化G时，G的目标是<br>$$<br>G^<em>=argmin_GV(G,D^</em>_G)<br>$$<br>GAN的极大极小博弈也就是：</p>
<p><img src="/2018/07/24/GAN/value%20function.png" alt="value function"> </p>
<p>GAN的训练过程可以看成是一个迭代过程，其中每次迭代包含：先固定G，最大化D，然后固定D，最小化G 。</p>
<h4 id="优化问题的推导"><a href="#优化问题的推导" class="headerlink" title="优化问题的推导"></a>优化问题的推导</h4><ol>
<li>证明固定G,  最优判别器为 $D_G^*(x)=\frac{P_{data}(x)}{P_{data}(x)+P_g(x)}$  ：</li>
</ol>
<p>值函数：<img src="/2018/07/24/GAN/value.png" alt="value"></p>
<p>可以写成积分形式：</p>
<p><img src="/2018/07/24/GAN/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-07-29%2014.51.00.png" alt="屏幕快照 2018-07-29 14.51.00"></p>
<p>利用积分换元可写成：</p>
<p><img src="/2018/07/24/GAN/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-07-29%2014.51.09.png" alt="屏幕快照 2018-07-29 14.51.09"></p>
<p>对这一项求极值，令一阶导=0 可得:<br>$$<br>D^<em>(x)=\frac{P_{data}(x)}{P_{data}(x)+P_g(x)}<br>$$<br>求二阶导，二阶导&lt;0,因此 $D^</em>(x)$ 为极大值。</p>
<p>最优判别器得证。</p>
<ol>
<li>当且仅当  $P_g(x)=P_{data}(x)$ 时，训练标准 C(G)=maxV(G,D) 的全局最小点可以达到，最小值为-log4。</li>
</ol>
<p>2.1  证明必要性：当  $P_g(x)=P_{data}(x)$ 时达到全局最小。</p>
<p>当$P_g(x)=P_{data}(x)$ 时，$D^*(x)=\frac{P_{data}(x)}{P_{data}(x)+P_g(x)}=\frac{1}{2}$ , 带入值函数V(G,D)得</p>
<p><img src="/2018/07/24/GAN/pr.png" alt="pr"></p>
<p>2.2  证明充分性：全局最小的条件是  $P_g(x)=P_{data}(x)$ </p>
<p>最优判别器 D* 代入到 $C(G)=max_V(G,D) $ 中： </p>
<p><img src="/2018/07/24/GAN/68126image (24" alt="68126image (../image/GAN/68126image%20(24).png)">.png)</p>
<p>可以利用已知最优值为-log4,和JS散度 构造函数：</p>
<p><img src="/2018/07/24/GAN/88447image (25" alt="88447image (../image/GAN/88447image%20(25).png)">.png)</p>
<p>化简后得：<img src="/2018/07/24/GAN/59249image (26" alt="59249image (../image/GAN/59249image%20(26).png)">.png)</p>
<p>根据概率密度的定义，$P_G,P_{data}$ 在积分域上的积分为1，可得</p>
<p><img src="/2018/07/24/GAN/85412image (27" alt="85412image (../image/GAN/85412image%20(27).png)">.png)</p>
<p>带入化简后的式子得：</p>
<p><img src="/2018/07/24/GAN/48004image (29" alt="48004image (../image/GAN/48004image%20(29).png)">.png)</p>
<p>根据KL散度的定义：同一个随机变量 x 有两个单独的概率分布 P(x) 和 Q(x)，用KL散度</p>
<p><img src="/2018/07/24/GAN/30228image (12" alt="30228image (../image/GAN/30228image%20(12).png)">.png)</p>
<p>来衡量这两个分布的差异。</p>
<p>根据KL散度的定义C(G)可以写成：<img src="/2018/07/24/GAN/68250image (30" alt="68250image (../image/GAN/68250image%20(30).png)">.png)</p>
<p>根据JS散度的定义：<br>$$<br>JSD(P||Q)=frac{1}{2}D(P||M )+frac{1}{2}D(Q||M )<br>其中，M=frac{1}{2}（P+Q）<br>$$<br>C(G) 可以写成：<img src="/2018/07/24/GAN/35494image (32" alt="35494image (../image/GAN/35494image%20(32).png)">.png)</p>
<p>JS散度在$P_{data}=P_G$ 是为0，因此全局最小的条件是  $P_g(x)=P_{data}(x)$</p>
<p>综合1、2 可以证明生成分布当且仅当等于真实数据分布式时，我们可以取得最优生成器。</p>
<p>以下部分多数摘自<a href="https://blog.csdn.net/qq_25737169/article/details/78857724" target="_blank" rel="noopener">https://blog.csdn.net/qq_25737169/article/details/78857724</a></p>
<p>对于有优缺点的分析，解释的比较容易理解。</p>
<h3 id="GAN的优点"><a href="#GAN的优点" class="headerlink" title="GAN的优点"></a>GAN的优点</h3><p>●  GAN是一种生成式模型，相比较其他生成模型（玻尔兹曼机和GSNs）只用到了反向传播,而不需要复杂的马尔科夫链</p>
<p>●  相比其他所有模型, GAN可以产生更加清晰，真实的样本</p>
<p>●  GAN采用的是一种无监督的学习方式训练，可以被广泛用在无监督学习和半监督学习领域</p>
<p>●  相比于变分自编码器, GANs没有引入任何决定性偏置( deterministic bias),变分方法引入决定性偏置,因为他们优化对数似然的下界,而不是似然度本身,这看起来导致了VAEs生成的实例比GANs更模糊</p>
<p>●  相比VAE, GANs没有变分下界,如果鉴别器训练良好,那么生成器可以完美的学习到训练样本的分布.换句话说,GANs是渐进一致的,但是VAE是有偏差的</p>
<p>●  GAN应用到一些场景上，比如图片风格迁移，超分辨率，图像补全，去噪，避免了损失函数设计的困难，不管三七二十一，只要有一个的基准，直接上判别器，剩下的就交给对抗训练了。</p>
<h3 id="存在的缺陷"><a href="#存在的缺陷" class="headerlink" title="存在的缺陷"></a>存在的缺陷</h3><p>●  训练GAN需要达到纳什均衡,有时候可以用梯度下降法做到,有时候做不到。我们还没有找到很好的达到纳什均衡的方法,所以训练GAN相比VAE或者PixelRNN是不稳定的,但我认为在实践中它还是比训练玻尔兹曼机稳定的多</p>
<p>●  GAN不适合处理离散形式的数据，比如文本</p>
<p>●  GAN存在训练不稳定、梯度消失、模式崩溃的问题</p>
<p><strong>模式崩溃(model collapse)原因</strong></p>
<p>一般出现在GAN训练不稳定的时候，具体表现为生成出来的结果非常差，但是即使加长训练时间后也无法得到很好的改善。</p>
<p>具体原因可以解释如下：GAN采用的是对抗训练的方式，G的梯度更新来自D，所以G生成的好不好，得看D怎么说。具体就是G生成一个样本，交给D去评判，D会输出生成的假样本是真样本的概率（0-1），相当于告诉G生成的样本有多大的真实性，G就会根据这个反馈不断改善自己，提高D输出的概率值。但是如果某一次G生成的样本可能并不是很真实，但是D给出了正确的评价，或者是G生成的结果中一些特征得到了D的认可，这时候G就会认为我输出的正确的，那么接下来我就这样输出肯定D还会给出比较高的评价，实际上G生成的并不怎么样，但是他们两个就这样自我欺骗下去了，导致最终生成结果缺失一些信息，特征不全。</p>
<p><strong>为什么GAN中的优化器不常用SGD</strong></p>
<ol>
<li><p>SGD容易震荡，容易使GAN训练不稳定，</p>
</li>
<li><p>GAN的目的是在高维非凸的参数空间中找到纳什均衡点，GAN的纳什均衡点是一个鞍点，但是SGD只会找到局部极小值，因为SGD解决的是一个寻找最小值的问题，GAN是一个博弈问题。</p>
</li>
</ol>
<p><strong>为什么GAN不适合处理文本数据</strong></p>
<ol>
<li>文本数据相比较图片数据来说是离散的，因为对于文本来说，通常需要将一个词映射为一个高维的向量，最终预测的输出是一个one-hot向量，假设softmax的输出是（0.2， 0.3， 0.1，0.2，0.15，0.05）那么变为onehot是（0，1，0，0，0，0），如果softmax输出是（0.2， 0.25， 0.2， 0.1，0.15，0.1 ），one-hot仍然是（0， 1， 0， 0， 0， 0），所以对于生成器来说，G输出了不同的结果但是D给出了同样的判别结果，并不能将梯度更新信息很好的传递到G中去，所以D最终输出的判别没有意义。</li>
<li>另外就是GAN的损失函数是JS散度，JS散度不适合衡量不想交分布之间的距离。</li>
</ol>
<p>（WGAN虽然使用wassertein距离代替了JS散度，但是在生成文本上能力还是有限，GAN在生成文本上的应用有seq-GAN,和强化学习结合的产物）</p>
<h3 id="训练GAN的一些技巧"><a href="#训练GAN的一些技巧" class="headerlink" title="训练GAN的一些技巧"></a>训练GAN的一些技巧</h3><ol>
<li>输入规范化到（-1，1）之间，最后一层的激活函数使用tanh（BEGAN除外）</li>
<li>使用wassertein GAN的损失函数，</li>
<li>如果有标签数据的话，尽量使用标签，也有人提出使用反转标签效果很好，另外使用标签平滑，单边标签平滑或者双边标签平滑</li>
<li>使用mini-batch norm， 如果不用batch norm 可以使用instance norm 或者weight norm</li>
<li>避免使用RELU和pooling层，减少稀疏梯度的可能性，可以使用leakrelu激活函数</li>
<li>优化器尽量选择ADAM，学习率不要设置太大，初始1e-4可以参考，另外可以随着训练进行不断缩小学习率，</li>
<li>给D的网络层增加高斯噪声，相当于是一种正则。</li>
</ol>
<h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><p><a href="https://www.jiqizhixin.com/articles/2017-10-1-1" target="_blank" rel="noopener">https://www.jiqizhixin.com/articles/2017-10-1-1</a></p>
<p><a href="https://blog.csdn.net/qq_25737169/article/details/78857724" target="_blank" rel="noopener">https://blog.csdn.net/qq_25737169/article/details/78857724</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://liuzhidanhhh.github.io/2018/04/19/利用twitter官网提供的api 爬取tweets/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="lzd">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Lzd's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/04/19/利用twitter官网提供的api 爬取tweets/" itemprop="url">利用twitter官网提供的api 爬取tweets.md</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-04-19T23:51:37+08:00">
                2018-04-19
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="利用twitter官网提供的api及tweepy库爬取tweets"><a href="#利用twitter官网提供的api及tweepy库爬取tweets" class="headerlink" title="利用twitter官网提供的api及tweepy库爬取tweets"></a>利用twitter官网提供的api及tweepy库爬取tweets</h3><p><a href="http://docs.tweepy.org/en/v3.5.0/" target="_blank" rel="noopener">tweepy官网文档</a></p>
<p><strong>思路：</strong><br>1.以用户为中心，爬取用户的所有推文数据<br>2.根据用户id寻找用户朋友的twitter id扩展待爬用户表<br>3.循环1,2</p>
<p><strong>几点说明：</strong><br>1.爬推特数据需要翻墙，推荐用ss。代码翻墙需要http，https代理。如果是socks的话会发现浏览器能翻墙，但是代码会提示</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tweepy.error.TweepError: Failed to send request: HTTPSConnectionPool(host=&apos;api.twitter.com&apos;, port=443): Max retries exceeded with url: ....</span><br></pre></td></tr></table></figure>
<p>说明https连接失败。如果需要终端翻墙参考<a href="http://www.cashqian.net/blog/001486989831982332565298e4942a2bb8f56b08f9d2475000" target="_blank" rel="noopener">Mac命令行终端下使用shadowsocks翻墙</a><br>然后在tweepy.API中加入代理信息，端口为你设定的代理端口。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">api = tweepy.API(auth, proxy=<span class="string">"127.0.0.1：1080"</span>,)</span><br></pre></td></tr></table></figure>
<p>2.使用官方api需要先申请一个应用程序以获得授权，申请地址<a href="https://apps.twitter.com/" target="_blank" rel="noopener">Twitter应用程序</a> 名字描述什么的随便写好好，没有审核时间，填写后即可获得consumer_key，consumer_secret，access_token，access_token_secret这些在求取数据时需要用到。</p>
<p>3.官方API有速率限制具体参见[<a href="https://developer.twitter.com/en/docs/basics/rate-limits" target="_blank" rel="noopener">Rate limits-Twitter Development</a>]授权用户和授权应用的请求窗口数有差异我用的。<code>user_timeline（）</code>状语从句：<code>user_friends（）</code>限制如下：<br><img src="/2018/04/19/利用twitter官网提供的api 爬取tweets/limit.png" alt="limit"></p>
<p>所以需要协调两个接口的调用频率。</p>
<p>4.当请求次数超过上限时会抛出异常，然后退出程序，解决方法时tweepy.API中将参数wait_on_rate_limit，wait_on_rate_limit_notify设置为True </p>
<p>到达上限时，程序将自动等待，并输出提示信息。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">api = tweepy.API(auth, proxy=<span class="string">"127.0.0.1：1080"</span>, wait_on_rate_limit=<span class="keyword">True</span>, wait_on_rate_limit_notify=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<p>5.api请求返回json格式数据如图：</p>
<p><img src="/2018/04/19/利用twitter官网提供的api 爬取tweets/json_format.png" alt="json_format"></p>
<p>6.有些用户设置不允许取数据时会提示<code>Not authorized.</code> 可以在异常部分处理异常，跳过改用户即可.tweepy.error信息也可以在上面的官方文档连接中查到。</p>
<p>代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br></pre></td><td class="code"><pre><span class="line">import tweepy</span><br><span class="line">import time</span><br><span class="line">import csv</span><br><span class="line">import threading</span><br><span class="line"></span><br><span class="line">consumer_key = &quot;xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&quot;</span><br><span class="line">consumer_secret = &apos;xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&apos;</span><br><span class="line">access_token = &apos;xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&apos;</span><br><span class="line">access_token_secret = &apos;xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&apos;</span><br><span class="line"></span><br><span class="line">lock = threading.Lock()</span><br><span class="line"></span><br><span class="line">def get_tweets():</span><br><span class="line">    global user_ids</span><br><span class="line">    global old_ids</span><br><span class="line">    lock.acquire()</span><br><span class="line">    try:</span><br><span class="line">        num = 0</span><br><span class="line">        while len(user_ids) &gt; 1:</span><br><span class="line">            try:</span><br><span class="line">                user_id = user_ids[num]</span><br><span class="line">                print(&apos;crawling user %s data...&apos; % user_id)</span><br><span class="line">                auth = tweepy.OAuthHandler(consumer_key, consumer_secret)</span><br><span class="line">                auth.set_access_token(access_token, access_token_secret)</span><br><span class="line">                api = tweepy.API(auth, proxy=&quot;127.0.0.1:1080&quot;, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)</span><br><span class="line">                tweets = []</span><br><span class="line">                new_tweets = api.user_timeline(user_id, count=200)</span><br><span class="line">                tweets.extend(new_tweets)</span><br><span class="line">                old = tweets[-1].id - 1</span><br><span class="line">                while len(new_tweets) &gt; 0:</span><br><span class="line">                    new_tweets = api.user_timeline(user_id=user_id, count=200, max_id=old)</span><br><span class="line">                    tweets.extend(new_tweets)</span><br><span class="line">                    old = tweets[-1].id - 1</span><br><span class="line">                    print(&apos;%s tweets downloaded&apos; % (len(tweets)))</span><br><span class="line"></span><br><span class="line">                out_tweets = [[tweet.id, tweet.text, tweet.created_at, tweet.lang, tweet.place, tweet.geo, tweet.source,</span><br><span class="line">                               tweet.truncated, tweet.favorite_count, tweet.favorited, tweet.in_reply_to_screen_name,</span><br><span class="line">                               tweet.in_reply_to_status_id, tweet.in_reply_to_user_id, tweet.is_quote_status,</span><br><span class="line">                               tweet.retweet_count, tweet.retweeted, tweet.user.id, tweet.user.name, tweet.user.screen_name,</span><br><span class="line">                               tweet.user.statuses_count, tweet.user.time_zone, tweet.user.url, tweet.user.notifications,</span><br><span class="line">                               tweet.user.profile_background_image_url, tweet.user.profile_image_url,</span><br><span class="line">                               tweet.user.profile_image_url_https, tweet.user.location, tweet.user.contributors_enabled,</span><br><span class="line">                               tweet.user.created_at, tweet.user.default_profile, tweet.user.default_profile_image,</span><br><span class="line">                               tweet.user.description, tweet.user.favourites_count, tweet.user.follow_request_sent,</span><br><span class="line">                               tweet.user.followers_count, tweet.user.following, tweet.user.friends_count,</span><br><span class="line">                               tweet.user.geo_enabled] for tweet in tweets]</span><br><span class="line">                user_ids.remove(user_id)</span><br><span class="line">                old_ids.append(user_id)</span><br><span class="line">                with open(&apos;./data1/%s_tweets.csv&apos; % user_id, &apos;w&apos;,encoding=&apos;utf-8&apos;) as file:</span><br><span class="line">                    writer = csv.writer(file)</span><br><span class="line">                    writer.writerows(out_tweets)</span><br><span class="line">                print(&apos;saved data&apos;)</span><br><span class="line">            except tweepy.TweepError as e:</span><br><span class="line">                if e.reason==&apos;Not authorized.&apos;:</span><br><span class="line">                    print(&apos;this user not authorized.&apos;)</span><br><span class="line">                    user_ids.remove(user_id)</span><br><span class="line">                    old_ids.append(user_id)</span><br><span class="line">                    continue</span><br><span class="line">                else:print(e)</span><br><span class="line">    finally:</span><br><span class="line">        lock.release()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_friends():</span><br><span class="line">    global user_ids</span><br><span class="line">    global old_ids</span><br><span class="line">    global oldest</span><br><span class="line">    lock.acquire()</span><br><span class="line">    try:</span><br><span class="line">        print(&apos;getting user friends id...&apos;)</span><br><span class="line">        auth = tweepy.OAuthHandler(consumer_key, consumer_secret)</span><br><span class="line">        auth.set_access_token(access_token, access_token_secret)</span><br><span class="line">        api = tweepy.API(auth, proxy=&quot;127.0.0.1:1080&quot;, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)</span><br><span class="line">        ids = []</span><br><span class="line">        for user in old_ids[10]:</span><br><span class="line">            try:</span><br><span class="line">                friends = api.friends_ids(user)</span><br><span class="line">                friend = []</span><br><span class="line">                for idd in friends:</span><br><span class="line">                    if (idd not in old_ids) and (idd not in user_ids) and(idd not in oldest):</span><br><span class="line">                        friend.append(idd)</span><br><span class="line">                ids.extend(friend)</span><br><span class="line"></span><br><span class="line">            except tweepy.TweepError as e:</span><br><span class="line">                if e.reason == &apos;Not authorized.&apos;:</span><br><span class="line">                    print(&apos;this user not authorized.&apos;)</span><br><span class="line">                    old_ids.remove(user)</span><br><span class="line">                    oldest.append(user)</span><br><span class="line">                    continue</span><br><span class="line">                else:</span><br><span class="line">                    print(e)</span><br><span class="line">            old_ids.remove(user)</span><br><span class="line">            oldest.append(user)</span><br><span class="line">        user_ids.extend(ids)</span><br><span class="line">        print(&apos;done!&apos;)</span><br><span class="line">        with open(&apos;crawled and expened user.txt&apos;,&apos;w&apos;,encoding=&apos;utf-8&apos;) as file:</span><br><span class="line">            for x in oldest:</span><br><span class="line">                file.write(str(x))</span><br><span class="line">                file.write(&apos; &apos;)</span><br><span class="line">    finally:</span><br><span class="line">        lock.release()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    user_ids = [25073877,198599889]</span><br><span class="line">    with open(&apos;old_ids.txt&apos;,&apos;r&apos;,encoding=&apos;utf-8&apos;) as file:</span><br><span class="line">        old_ids=[x for x in file.read().split(&apos; &apos;)]</span><br><span class="line">    while len(user_ids) &gt; 0:</span><br><span class="line">        t1=threading.Thread(target=get_tweets)</span><br><span class="line">        t2=threading.Thread(target=get_friends)</span><br><span class="line">        t1.start()</span><br><span class="line">        t1.join()</span><br><span class="line">        t2.start()</span><br><span class="line">        t2.join()</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">lzd</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">tags</span>
                
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">lzd</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>

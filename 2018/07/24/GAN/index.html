<!DOCTYPE html>
<html>
<head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">
    <meta name="keywords" content="hexo, autumn">
    <title>
        Lzd&#39;s blog
    </title>
    <!-- favicon -->
    
    <link rel="icon" href="http://osly086qe.bkt.clouddn.com/favicon.ico">
    
    <link rel="stylesheet" href="/css/style.css">

    <!-- highlight -->
    <link rel="stylesheet" href="https://cdn.bootcss.com/highlight.js/9.12.0/styles/github-gist.min.css">
    <script src="//cdn.bootcss.com/highlight.js/9.2.0/highlight.min.js"></script>
    <script>
        hljs.initHighlightingOnLoad()
    </script>
    <script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script>
    <link rel="stylesheet" href="http://osly086qe.bkt.clouddn.com/hexo-infinite-scroll-v1.0.1.min.css">
    <script src="http://osly086qe.bkt.clouddn.com/hexo-infinite-scroll-v1.0.3.min.js"></script>
    <script>
        infiniteScroll()

        // for mobile menu
        $(function() {
            $('.social-button').click(function() {
                if ($('.social-links').hasClass('hide-links')) {
                    $('.social-links').removeClass('hide-links')
                } else {
                    $('.social-links').addClass('hide-links')
                }
            })
        })
    </script>
</head>

    <body style="background: url(http://osly086qe.bkt.clouddn.com/button-bg.png) #f3f3f3">
        <div class="container">
            <header class="header">
    <h1 class="title">
        <a href="/" class="logo">
            Lzd&#39;s blog
        </a>
    </h1>
    <h2 class="desc">
        
    </h2>

    <nav class="links">
        <button class="social-button">
            menu
        </button>
        <ul class="social-links hide-links">
            
                <li>
                    <a href="https://github.com/FrontendSophie">
                        Github
                    </a>
                </li>
                
                <li>
                    <a href="https://www.linkedin.com/in/frontendsophie/">
                        LinkedIn
                    </a>
                </li>
                
        </ul>
    </nav>
</header>
                <main class="main">
                    <article class="post">
    
            <h2 class="post-title">
                GAN（Generative Adversarial Networks）
            </h2>
            <ul class="post-date">
                <li>
                    2018-07-24
                </li>
                <li>
                    lzd
                </li>
            </ul>
            <div class="post-content">
                <h2 id="GAN（Generative-Adversarial-Networks）"><a href="#GAN（Generative-Adversarial-Networks）" class="headerlink" title="GAN（Generative Adversarial Networks）"></a>GAN（Generative Adversarial Networks）</h2><p>论文链接：<a href="https://arxiv.org/abs/1406.2661" target="_blank" rel="noopener">https://arxiv.org/abs/1406.2661</a></p>
<h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>GAN 对抗式生成网络，主要是一种博弈思想。主要包含两个网络：判别网络D、生成网络G。</p>
<p>在这里判别网络可以看成是一个鉴赏家，生成网络可以看成是一个画家。判别网络D需要鉴定画作是出自新手画家G的，还是来自绘画大师的作品。G开始是一个新手画家，在和鉴赏家博弈的过程中，不断的提高自己的画技以欺骗鉴赏家，鉴赏家同时也在不断的提升自己的鉴赏能力。</p>
<h3 id="原理推导"><a href="#原理推导" class="headerlink" title="原理推导"></a>原理推导</h3><p>说明：原论文的推导跳过了一些细节，可以参考参考<a href="https://www.jiqizhixin.com/articles/2017-10-1-1" target="_blank" rel="noopener">机器之心</a>推导，但是机器之心有些地方的描述，比较生硬的翻译了原论文，不是很好理解，可以看原论文，下面的原理部分是我结合两者，并加入了自己的理解。</p>
<h4 id="宏观上的理解"><a href="#宏观上的理解" class="headerlink" title="宏观上的理解"></a>宏观上的理解</h4><p>GAN实际上是一个D、G的极大极小博弈。存在一个真实分布$P_{ data }( x )$  和生成分布 $P_g(x)$ ,生成器G的目的是让生成分布 $ P_g ( x )$ 靠近真实分布$ P_{ data } ( x )$ ，而判别器D的目的是区分两个分布。</p>
<p>我们需要的是这个可以以假乱真的生成网络。最优情况是$P_g(x)=P_{data}(x)$ ,此时D(x)=0.5。</p>
<p>下图GAN 的原理示意图：</p>
<p><img src="/2018/07/24/GAN/GANdemo.png" alt="GANdemo"></p>
<p>黑色虚线的分布是真实分布，绿色的分布是生成分布，蓝色虚线是判别分布，z是随机噪声，通过生成器将z映射到数据空间 x=G(z)，GAN会训练并更新判别分布（蓝色），指引生成分布向真实分布靠拢。</p>
<p>D、G的最大最小博弈值函数函数V(G,D):</p>
<p><img src="/2018/07/24/GAN/value%20function.png" alt="value function"></p>
<p>D(x)表示x来自真实数据集，而非生成数据集。训练D以最好的区分样本来自真实数据和生成数据，也就是最大化D(x),同时训练G以最小化log(1-D(G(z))), 可直观的理解为训练G生成的质量来使得D(G(z)) 最大，也就是log(1-D(G(z)))最小。</p>
<p>原论文表明在非参数限制的情况下可以得到最优解，这里的非参数限制，我不是很理解。</p>
<h4 id="GAN算法："><a href="#GAN算法：" class="headerlink" title="GAN算法："></a>GAN算法：</h4><p><img src="/2018/07/24/GAN/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-07-29%2010.43.54.png" alt="屏幕快照 2018-07-29 10.43.54"></p>
<p>说明：</p>
<ol>
<li>在收敛点附近的对抗训练$P_r$ 和$P_g$  相似，D是一个局部准确的分类器。</li>
<li>在算法先先训练k次D，再训练G，在k次内循环中，训练D来区分$P_{data}$ 和$P_g$ ，收敛到 $D^*(x)=\frac{P_{data}(x)}{P_{data}(x)+P_g(x)}$ </li>
<li>接着固定D，训练G，D的梯度引导着G的生成分布向真实分布靠近。</li>
<li>在复杂度足够的情况下(什么情况是足够的？)D、G会达到一个均衡点 $P_g(x)=P_{data}(x)$ ,此时D(x)=0.5。</li>
</ol>
<h4 id="GAN原理形式化的定义及说明"><a href="#GAN原理形式化的定义及说明" class="headerlink" title="GAN原理形式化的定义及说明"></a>GAN原理形式化的定义及说明</h4><p>GAN的目标是：生成器生成与真实数据几乎没有区别的样本，即将随机变量生成为某一种概率分布，也可以说概率密度函数为相等的。这可以通过构造最优化问题来解决，即定义一个最优化问题，其中最优生成器 G 满足$ P_G(x)=P_{data}(x)$。如果我们知道求解的 G 最后会满足该关系，那么我们就可以合理地期望神经网络通过典型的 SGD 训练就能得到最优的 G。</p>
<p>作者将最优化问题定义为一个值函数V(G&lt;D) ：</p>
<p><img src="/2018/07/24/GAN/value.png" alt="value"></p>
<p>其中第一项是定义一个判别器 D 以判别样本是不是从 $ P_{data}(x) $ 分布中取出来的，最大化这一项相当于令判别器 D 在 x 服从于 data 的概率密度时能准确地预测 D(x)=1。第二项是企图欺骗判别器的生成器 G，$D(G(z))\in(-1,1)$ , log(1-D(G(z)))&lt;0, 当G没有欺骗到D时，第二项的均值接近0，因此，判别器的目标为最大化上面值函数。</p>
<p>给定生成器G，最优判别器可以表示为：<br>$$<br>D^<em>_G=argmax_DV(G,D)<br>$$<br>当$D=D^</em>_G$ 固定，优化G时，G的目标是<br>$$<br>G^<em>=argmin_GV(G,D^</em>_G)<br>$$<br>GAN的极大极小博弈也就是：</p>
<p><img src="/2018/07/24/GAN/value%20function.png" alt="value function"> </p>
<p>GAN的训练过程可以看成是一个迭代过程，其中每次迭代包含：先固定G，最大化D，然后固定D，最小化G 。</p>
<h4 id="优化问题的推导"><a href="#优化问题的推导" class="headerlink" title="优化问题的推导"></a>优化问题的推导</h4><ol>
<li>证明固定G,  最优判别器为 $D_G^*(x)=\frac{P_{data}(x)}{P_{data}(x)+P_g(x)}$  ：</li>
</ol>
<p>值函数：<img src="/2018/07/24/GAN/value.png" alt="value"></p>
<p>可以写成积分形式：</p>
<p><img src="/2018/07/24/GAN/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-07-29%2014.51.00.png" alt="屏幕快照 2018-07-29 14.51.00"></p>
<p>利用积分换元可写成：</p>
<p><img src="/2018/07/24/GAN/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-07-29%2014.51.09.png" alt="屏幕快照 2018-07-29 14.51.09"></p>
<p>对这一项求极值，令一阶导=0 可得:<br>$$<br>D^<em>(x)=\frac{P_{data}(x)}{P_{data}(x)+P_g(x)}<br>$$<br>求二阶导，二阶导&lt;0,因此 $D^</em>(x)$ 为极大值。</p>
<p>最优判别器得证。</p>
<ol>
<li>当且仅当  $P_g(x)=P_{data}(x)$ 时，训练标准 C(G)=maxV(G,D) 的全局最小点可以达到，最小值为-log4。</li>
</ol>
<p>2.1  证明必要性：当  $P_g(x)=P_{data}(x)$ 时达到全局最小。</p>
<p>当$P_g(x)=P_{data}(x)$ 时，$D^*(x)=\frac{P_{data}(x)}{P_{data}(x)+P_g(x)}=\frac{1}{2}$ , 带入值函数V(G,D)得</p>
<p><img src="/2018/07/24/GAN/pr.png" alt="pr"></p>
<p>2.2  证明充分性：全局最小的条件是  $P_g(x)=P_{data}(x)$ </p>
<p>最优判别器 D* 代入到 $C(G)=max_V(G,D) $ 中： </p>
<p><img src="/2018/07/24/GAN/68126image (24" alt="68126image (../image/GAN/68126image%20(24).png)">.png)</p>
<p>可以利用已知最优值为-log4,和JS散度 构造函数：</p>
<p><img src="/2018/07/24/GAN/88447image (25" alt="88447image (../image/GAN/88447image%20(25).png)">.png)</p>
<p>化简后得：<img src="/2018/07/24/GAN/59249image (26" alt="59249image (../image/GAN/59249image%20(26).png)">.png)</p>
<p>根据概率密度的定义，$P_G,P_{data}$ 在积分域上的积分为1，可得</p>
<p><img src="/2018/07/24/GAN/85412image (27" alt="85412image (../image/GAN/85412image%20(27).png)">.png)</p>
<p>带入化简后的式子得：</p>
<p><img src="/2018/07/24/GAN/48004image (29" alt="48004image (../image/GAN/48004image%20(29).png)">.png)</p>
<p>根据KL散度的定义：同一个随机变量 x 有两个单独的概率分布 P(x) 和 Q(x)，用KL散度</p>
<p><img src="/2018/07/24/GAN/30228image (12" alt="30228image (../image/GAN/30228image%20(12).png)">.png)</p>
<p>来衡量这两个分布的差异。</p>
<p>根据KL散度的定义C(G)可以写成：<img src="/2018/07/24/GAN/68250image (30" alt="68250image (../image/GAN/68250image%20(30).png)">.png)</p>
<p>根据JS散度的定义：<br>$$<br>JSD(P||Q)=frac{1}{2}D(P||M )+frac{1}{2}D(Q||M )<br>其中，M=frac{1}{2}（P+Q）<br>$$<br>C(G) 可以写成：<img src="/2018/07/24/GAN/35494image (32" alt="35494image (../image/GAN/35494image%20(32).png)">.png)</p>
<p>JS散度在$P_{data}=P_G$ 是为0，因此全局最小的条件是  $P_g(x)=P_{data}(x)$</p>
<p>综合1、2 可以证明生成分布当且仅当等于真实数据分布式时，我们可以取得最优生成器。</p>
<p>以下部分多数摘自<a href="https://blog.csdn.net/qq_25737169/article/details/78857724" target="_blank" rel="noopener">https://blog.csdn.net/qq_25737169/article/details/78857724</a></p>
<p>对于有优缺点的分析，解释的比较容易理解。</p>
<h3 id="GAN的优点"><a href="#GAN的优点" class="headerlink" title="GAN的优点"></a>GAN的优点</h3><p>●  GAN是一种生成式模型，相比较其他生成模型（玻尔兹曼机和GSNs）只用到了反向传播,而不需要复杂的马尔科夫链</p>
<p>●  相比其他所有模型, GAN可以产生更加清晰，真实的样本</p>
<p>●  GAN采用的是一种无监督的学习方式训练，可以被广泛用在无监督学习和半监督学习领域</p>
<p>●  相比于变分自编码器, GANs没有引入任何决定性偏置( deterministic bias),变分方法引入决定性偏置,因为他们优化对数似然的下界,而不是似然度本身,这看起来导致了VAEs生成的实例比GANs更模糊</p>
<p>●  相比VAE, GANs没有变分下界,如果鉴别器训练良好,那么生成器可以完美的学习到训练样本的分布.换句话说,GANs是渐进一致的,但是VAE是有偏差的</p>
<p>●  GAN应用到一些场景上，比如图片风格迁移，超分辨率，图像补全，去噪，避免了损失函数设计的困难，不管三七二十一，只要有一个的基准，直接上判别器，剩下的就交给对抗训练了。</p>
<h3 id="存在的缺陷"><a href="#存在的缺陷" class="headerlink" title="存在的缺陷"></a>存在的缺陷</h3><p>●  训练GAN需要达到纳什均衡,有时候可以用梯度下降法做到,有时候做不到。我们还没有找到很好的达到纳什均衡的方法,所以训练GAN相比VAE或者PixelRNN是不稳定的,但我认为在实践中它还是比训练玻尔兹曼机稳定的多</p>
<p>●  GAN不适合处理离散形式的数据，比如文本</p>
<p>●  GAN存在训练不稳定、梯度消失、模式崩溃的问题</p>
<p><strong>模式崩溃(model collapse)原因</strong></p>
<p>一般出现在GAN训练不稳定的时候，具体表现为生成出来的结果非常差，但是即使加长训练时间后也无法得到很好的改善。</p>
<p>具体原因可以解释如下：GAN采用的是对抗训练的方式，G的梯度更新来自D，所以G生成的好不好，得看D怎么说。具体就是G生成一个样本，交给D去评判，D会输出生成的假样本是真样本的概率（0-1），相当于告诉G生成的样本有多大的真实性，G就会根据这个反馈不断改善自己，提高D输出的概率值。但是如果某一次G生成的样本可能并不是很真实，但是D给出了正确的评价，或者是G生成的结果中一些特征得到了D的认可，这时候G就会认为我输出的正确的，那么接下来我就这样输出肯定D还会给出比较高的评价，实际上G生成的并不怎么样，但是他们两个就这样自我欺骗下去了，导致最终生成结果缺失一些信息，特征不全。</p>
<p><strong>为什么GAN中的优化器不常用SGD</strong></p>
<ol>
<li><p>SGD容易震荡，容易使GAN训练不稳定，</p>
</li>
<li><p>GAN的目的是在高维非凸的参数空间中找到纳什均衡点，GAN的纳什均衡点是一个鞍点，但是SGD只会找到局部极小值，因为SGD解决的是一个寻找最小值的问题，GAN是一个博弈问题。</p>
</li>
</ol>
<p><strong>为什么GAN不适合处理文本数据</strong></p>
<ol>
<li>文本数据相比较图片数据来说是离散的，因为对于文本来说，通常需要将一个词映射为一个高维的向量，最终预测的输出是一个one-hot向量，假设softmax的输出是（0.2， 0.3， 0.1，0.2，0.15，0.05）那么变为onehot是（0，1，0，0，0，0），如果softmax输出是（0.2， 0.25， 0.2， 0.1，0.15，0.1 ），one-hot仍然是（0， 1， 0， 0， 0， 0），所以对于生成器来说，G输出了不同的结果但是D给出了同样的判别结果，并不能将梯度更新信息很好的传递到G中去，所以D最终输出的判别没有意义。</li>
<li>另外就是GAN的损失函数是JS散度，JS散度不适合衡量不想交分布之间的距离。</li>
</ol>
<p>（WGAN虽然使用wassertein距离代替了JS散度，但是在生成文本上能力还是有限，GAN在生成文本上的应用有seq-GAN,和强化学习结合的产物）</p>
<h3 id="训练GAN的一些技巧"><a href="#训练GAN的一些技巧" class="headerlink" title="训练GAN的一些技巧"></a>训练GAN的一些技巧</h3><ol>
<li>输入规范化到（-1，1）之间，最后一层的激活函数使用tanh（BEGAN除外）</li>
<li>使用wassertein GAN的损失函数，</li>
<li>如果有标签数据的话，尽量使用标签，也有人提出使用反转标签效果很好，另外使用标签平滑，单边标签平滑或者双边标签平滑</li>
<li>使用mini-batch norm， 如果不用batch norm 可以使用instance norm 或者weight norm</li>
<li>避免使用RELU和pooling层，减少稀疏梯度的可能性，可以使用leakrelu激活函数</li>
<li>优化器尽量选择ADAM，学习率不要设置太大，初始1e-4可以参考，另外可以随着训练进行不断缩小学习率，</li>
<li>给D的网络层增加高斯噪声，相当于是一种正则。</li>
</ol>
<h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><p><a href="https://www.jiqizhixin.com/articles/2017-10-1-1" target="_blank" rel="noopener">https://www.jiqizhixin.com/articles/2017-10-1-1</a></p>
<p><a href="https://blog.csdn.net/qq_25737169/article/details/78857724" target="_blank" rel="noopener">https://blog.csdn.net/qq_25737169/article/details/78857724</a></p>

            </div>
</article>
                </main>
                <aside class="aside">
                    <section class="aside-section">
                        
    <h1>Categories</h1>

    

                    </section>
                    <section class="aside-section">
                        
    <h1>Archives</h1>

    <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/">2018 . 07</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/">2018 . 04</a></li></ul>

                    </section>
                    <section class="aside-section tag">
                        
    <h1>Tags</h1>

    <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/GAN、WGAN、DCGAN、EEG-GAN-系列介绍/">GAN、WGAN、DCGAN、EEG GAN 系列介绍</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python-tweepy/">python tweepy</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/神经网络/">神经网络</a></li></ul>

                    </section>
                </aside>
        </div>
    </body>

</html>